{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from n2c2_tokenizer import build_n2c2_tokenizer #University of Utah code. Credit to Kelly and Jianlin\n",
    "import time, os, sys, multiprocessing, nltk, itertools\n",
    "from multiprocessing import Pool\n",
    "from sqlalchemy import create_engine, MetaData, Table, select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--> Prepare input data and a vocabulary file.\n",
    "Train the biLM.\n",
    "Test (compute the perplexity of) the biLM on heldout data.\n",
    "Write out the weights from the trained biLM to a hdf5 file.\n",
    "See the instructions above for using the output from Step #4 in downstream models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building n2c2 tokenizer...\n",
      "('.', '!')\n",
      "Enabling NLTK Punkt for sentence tokenization...\n",
      "Type of sentence tokenizer : <class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n",
      "Enabling custom preprocessing expressions.  Total : 8\n",
      "Class type initialized for ClinicalSentenceTokenizer for sentence tokenization : <class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n",
      "Compiled 8 total preprocessing regular expressions\n",
      "Class type initialized for IndexTokenizer for sentence tokenization: <class 'clinical_tokenizers.ClinicalSentenceTokenizer'>\n",
      "[['I', 'am', 'a', 'simple', 'document', '.'], ['here', 'are', 'my', 'sentences', '.'], ['nlp', 'is', 'the', 'best', '.']]\n"
     ]
    }
   ],
   "source": [
    "'''Step 0: Initalize our tokenizer for MIMIC data'''\n",
    "\n",
    "ENABLE_PYRUSH_SENTENCE_TOKENIZER = False\n",
    "\n",
    "n2c2_tokenizer = build_n2c2_tokenizer(enable_pyrush_sentence_tokenizer = ENABLE_PYRUSH_SENTENCE_TOKENIZER,\n",
    "                                     disable_custom_preprocessing = ENABLE_PYRUSH_SENTENCE_TOKENIZER, keep_token_strings=True)\n",
    "\n",
    "tokenized_doc_example = n2c2_tokenizer.tokenize_document(\"I am a simple document. here are my sentences. nlp is the best.\")\n",
    "\n",
    "print(tokenized_doc_example.sentence_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Step 1: Load the Mimic data. I have my Mimic data in an sqlite database. \n",
    "For how to do this, see: https://github.com/hclent/PyPatent/blob/master/readMimic.py'''\n",
    "\n",
    "def getMimicTexts():\n",
    "    '''\n",
    "    Input: N/A\n",
    "    Output: List[Strings] for all 2 million+ MIMIC texts **lowercase**. \n",
    "    We're going to use this List[Strings] to create the set of vocabulary words that is needed for BiLM.\n",
    "    '''\n",
    "    t1 = time.time() #start timer\n",
    "    \n",
    "    engine = create_engine('sqlite:///mimic.db') #initiated database engine\n",
    "    conn = engine.connect()\n",
    "    metadata = MetaData(bind=engine) #init metadata. will be empty\n",
    "    metadata.reflect(engine) #retrieve db info for metadata (tables, columns, types)\n",
    "    mydata = Table('mydata', metadata)\n",
    "\n",
    "    data: list[string] = []\n",
    "\n",
    "    #Query db for text. Not efficient. You can only execute one statment at a time with sqllite. Soz bro.   \n",
    "    s = select([mydata.c.TEXT]) \n",
    "    print(type(s))\n",
    "    result = conn.execute(s)\n",
    "    print(type(result))\n",
    "    for row in result:\n",
    "        #text\n",
    "        the_text = row[\"TEXT\"]\n",
    "        keep_text = the_text.rstrip()\n",
    "        lower_text = keep_text.lower() #lowercase v important.\n",
    "        # NB: tokenization will happen later. It is too slow to *NOT* run in parallel. \n",
    "        data.append(lower_text)\n",
    "    \n",
    "    print(\" * Finished step0: done in %0.3fs.\" % (time.time() - t1))\n",
    "    #Takes less than 1 minute. \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sqlalchemy.sql.selectable.Select'>\n",
      "<class 'sqlalchemy.engine.result.ResultProxy'>\n",
      " * Finished step0: done in 48.217s.\n"
     ]
    }
   ],
   "source": [
    "list_of_all_docs = getMimicTexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* list_of_all_docs is a:  <class 'list'>\n",
      "* number docs in list_of_all_docs:  2083180\n",
      "* documents in list_of_all_docs are:  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#### Example ##### \n",
    "print(\"* list_of_all_docs is a: \", type(list_of_all_docs))\n",
    "print(\"* number docs in list_of_all_docs: \", len(list_of_all_docs))\n",
    "print(\"* documents in list_of_all_docs are: \", type(list_of_all_docs[0]))\n",
    "# print(\"* Example documents: \", list_of_all_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: we also want to output data.txt as a nice string so should probably output that here honestly.\n",
    "'''Step 2: Create a helper function to run with multiprocessing that will tokenize the document, \n",
    "create the set of tokens, and format the sentences for output to data.txt.'''\n",
    "def getSetOfWords(document):\n",
    "    '''\n",
    "    Input: String of the document\n",
    "    Output: Set(Strings) = This will output the set of tokens in the document\n",
    "    TODO: We also should create the nice, pretty strings for data.txt here ...\n",
    "    '''\n",
    "    #tokenize\n",
    "    tokenized = n2c2_tokenizer.tokenize_document(document).sentence_tokens_list #list of lists of tokens\n",
    "    #format sentences for data.txt\n",
    "    pretty_sentences = [' '.join(sentences) for sentences in tokenized]\n",
    "    \n",
    "    #flatten the list of lists into one list of strings \n",
    "    flatten = list(itertools.chain(*tokenized))\n",
    "    \n",
    "    '''\n",
    "    PROBLEM:\n",
    "    IMPORTANT: the vocabulary file should be sorted in descending order by token count in your training data. \n",
    "    The first three lines should be the special tokens (<S>, </S> and <UNK>), \n",
    "    ****then the most common token in the training data, ending with the least common token.****\n",
    "    I didn;t see this before! Now we need to count things goddammit \n",
    "    '''\n",
    "    \n",
    "    #create the set\n",
    "    unique_words: set = set(flatten)\n",
    "    return_dict = {'set': unique_words, 'sentences': pretty_sentences}\n",
    "        \n",
    "    return return_dict\n",
    "\n",
    "unique_words_example = getSetOfWords(list_of_all_docs[0])\n",
    "#print(unique_words_example[\"set\"])\n",
    "#print(\"#\"*20)\n",
    "#print(unique_words_example[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* created worker pools\n",
      "* initialized map_async to naiveSearchText function with docs\n",
      "* did map to getSetOfWords function with docs. WITH async\n",
      "* closed pool\n",
      "* joined pool\n",
      "Number of dictionaries created:  10000\n",
      "208663\n",
      " * Created mimic_data.txt & mimic_vocab.txt: done in 57.567s.\n"
     ]
    }
   ],
   "source": [
    "'''Step 3: Run the helper function asynchronously with multiprocessing to create the vocab.txt and data.txt \n",
    "that is necessary to run BiLM.\n",
    "'''\n",
    "def createVocabFile():\n",
    "    '''\n",
    "    Input: String = Point it at the text file that contains all of the mimic files!\n",
    "    '''\n",
    "    t1 = time.time() #start the timer\n",
    "    \n",
    "    pool_size = multiprocessing.cpu_count() #NOTE: Usin' all yer CPU's my friend. Change this if you want.\n",
    "    pool = Pool(pool_size)\n",
    "    print('* created worker pools')\n",
    "    results0 = pool.map_async(getSetOfWords, list_of_all_docs[0:10000])  #TODO: this will be the whole set, not a subset\n",
    "    print('* initialized map_async to naiveSearchText function with docs')\n",
    "    print('* did map to getSetOfWords function with docs. WITH async')\n",
    "    pool.close()\n",
    "    print('* closed pool')\n",
    "    pool.join()\n",
    "    print('* joined pool')\n",
    "    list_of_dicts = [r for r in results0.get() if r is not None] # A BUNCH OF SETS\n",
    "    print(\"Number of dictionaries created: \", len(list_of_dicts))\n",
    "\n",
    "    \"\"\"Step A: create data.txt: Should have 1 sentence per line\"\"\"\n",
    "    #get all doc's sentences\n",
    "    document_sentences = [s[\"sentences\"]  for s in list_of_dicts]\n",
    "    #flatten to one big list of sentences\n",
    "    flatten_sents: list[string] = list(itertools.chain(*document_sentences))\n",
    "    #output to data_vocab.txt\n",
    "    with open(\"mimic_data.txt\", \"w\") as out:\n",
    "        for sent in flatten_sents:\n",
    "            out.write(sent)\n",
    "            out.write(\"\\n\")\n",
    "    \n",
    "    \"\"\"Step B: create vocab.txt: Should have 1 token per line, as well as AllenNLP special tokens.\"\"\"\n",
    "    #init a set\n",
    "    giga_set = set()\n",
    "    #now add all of the sets to it to create one super big set. \n",
    "    #This could probably be done better with recursion, if anyone wants to update it and submit a pull request :) \n",
    "    quick_and_dirty = [giga_set.update(s[\"set\"]) for s in list_of_dicts]\n",
    "    print(len(giga_set))\n",
    "    #print(giga_set) #haha lets not actually print this omg\n",
    "    \n",
    "    #we also need to add these AllenNLP specific things\n",
    "    allen_specific = set(['<S>','</S>','<UNK>'])\n",
    "    giga_set.update(allen_specific)\n",
    "\n",
    "    #now output to vocab.txt\n",
    "    with open(\"mimic_vocab.txt\", \"w\") as out:\n",
    "        for word in giga_set:\n",
    "            out.write(word)\n",
    "            out.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "    print(\" * Created mimic_data.txt & mimic_vocab.txt: done in %0.3fs.\" % (time.time() - t1))\n",
    "\n",
    "\n",
    "createVocabFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 docs in 0.182s.\n",
    "# 100 docs in 0.696s.\n",
    "# 1,000 docs in 10.533s.\n",
    "# 10,000 docs in 57.567s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare input data and a vocabulary file.\n",
    "--> Train the biLM.\n",
    "Test (compute the perplexity of) the biLM on heldout data.\n",
    "Write out the weights from the trained biLM to a hdf5 file.\n",
    "See the instructions above for using the output from Step #4 in downstream models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
