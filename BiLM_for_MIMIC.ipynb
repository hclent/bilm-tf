{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training `ELMo` on MIMIC data for Clinical Natural Language Processing\n",
    "\n",
    "Short tutorial on how to train [AllenNLP's ELMo](https://allennlp.org/elmo) on [MIMIC](https://mimic.physionet.org/) data for medical/clinical Natural Language Processing. MIMIC is the most commonly used dataset for people doing NLP for medical/clinical purposes. Because CITI training is required to access MIMIC data, I will not be sharing the training files or the final model, however this tutorial will guide you through the steps to train `ELMo` on MIMIC data yourself! Training `ELMo` from scratch is necessary for medical/clinical NLP, because the language we find in clinical notes and medical writeupts is vastly differant than the language of Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To get started, `git clone` [this repository](https://github.com/hclent/bilm-tf), which is a fork of the original `BiLM` repository plus a copy of this jupyter notebook to generate the training data for ELMo from MIMIC. \n",
    "\n",
    "Then set up your Python environment. I'd recommend creating a new anaconda environment with our `requirements.txt`.\n",
    "Once you have done this, don't forgt to also follow the installation instructions for `BiLM`:\n",
    "\n",
    "    pip install tensorflow-gpu==1.2 h5py\n",
    "    python setup.py install \n",
    "    \n",
    "And I would suggest running the tests: `python -m unittest discover tests/` \n",
    "\n",
    "To train `ELMo` on MIMIC, we will complete the following steps from the `BiLM README`:\n",
    "\n",
    "\n",
    "    Prepare input data and a vocabulary file.\n",
    "    Train the biLM.\n",
    "    Test (compute the perplexity of) the biLM on heldout data.\n",
    "    Write out the weights from the trained biLM to a hdf5 file.\n",
    "    See the instructions above for using the output from Step #4 in downstream models.\n",
    "\n",
    "\n",
    "Let's get started by preparing the input data and vocabulary file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from n2c2_tokenizer import build_n2c2_tokenizer #Credit to Kelly (https://github.com/burgersmoke) and Jianlin (https://github.com/jianlins)\n",
    "import time, os, sys, multiprocessing, nltk, itertools\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "from sqlalchemy import create_engine, MetaData, Table, select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building n2c2 tokenizer...\n",
      "('.', '!')\n",
      "Enabling NLTK Punkt for sentence tokenization...\n",
      "Type of sentence tokenizer : <class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n",
      "Enabling custom preprocessing expressions.  Total : 8\n",
      "Class type initialized for ClinicalSentenceTokenizer for sentence tokenization : <class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n",
      "Compiled 8 total preprocessing regular expressions\n",
      "Class type initialized for IndexTokenizer for sentence tokenization: <class 'clinical_tokenizers.ClinicalSentenceTokenizer'>\n",
      "[['I', 'am', 'a', 'simple', 'document', '.'], ['Here', 'are', 'my', 'sentences', '.'], ['NLP', 'is', 'the', 'best', '.']]\n"
     ]
    }
   ],
   "source": [
    "'''Step 0: Initalize our tokenizer for MIMIC data'''\n",
    "\n",
    "ENABLE_PYRUSH_SENTENCE_TOKENIZER = False\n",
    "\n",
    "n2c2_tokenizer = build_n2c2_tokenizer(enable_pyrush_sentence_tokenizer = ENABLE_PYRUSH_SENTENCE_TOKENIZER,\n",
    "                                     disable_custom_preprocessing = ENABLE_PYRUSH_SENTENCE_TOKENIZER, keep_token_strings=True)\n",
    "#Here is an example:\n",
    "tokenized_doc_example = n2c2_tokenizer.tokenize_document(\"I am a simple document. Here are my sentences. NLP is the best.\")\n",
    "\n",
    "print(tokenized_doc_example.sentence_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Step 1: Load the Mimic data. I have my Mimic data in an sqlite database. \n",
    "For how to do this, see: https://github.com/hclent/PyPatent/blob/master/readMimic.py'''\n",
    "\n",
    "def getMimicTexts():\n",
    "    '''\n",
    "    Input: N/A\n",
    "    Output: List[Strings] for all 2 million+ MIMIC texts **lowercase**. \n",
    "    We're going to use this List[Strings] to generate the data and sorted vocabulary files that are needed to train BiLM.\n",
    "    '''\n",
    "    t1 = time.time() #start timer\n",
    "    \n",
    "    engine = create_engine('sqlite:///mimic.db') #initiated database engine\n",
    "    conn = engine.connect()\n",
    "    metadata = MetaData(bind=engine) #init metadata. will be empty\n",
    "    metadata.reflect(engine) #retrieve db info for metadata (tables, columns, types)\n",
    "    mydata = Table('mydata', metadata)\n",
    "\n",
    "    data: list[string] = []\n",
    "\n",
    "    #Query db for text. Not efficient. You can only execute one statment at a time with sqllite. Soz bro.   \n",
    "    s = select([mydata.c.TEXT]) \n",
    "    print(type(s))\n",
    "    result = conn.execute(s)\n",
    "    print(type(result))\n",
    "    for row in result:\n",
    "        #text\n",
    "        the_text = row[\"TEXT\"]\n",
    "        keep_text = the_text.rstrip()\n",
    "        lower_text = keep_text.lower() #lowercase v important.\n",
    "        # NB: tokenization will happen later. It is too slow to *NOT* run in parallel. \n",
    "        data.append(lower_text)\n",
    "    \n",
    "    print(\" * Finished step0: done in %0.3fs.\" % (time.time() - t1))\n",
    "    #Takes less than 1 minute to load all MIMIC texts into memory.\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sqlalchemy.sql.selectable.Select'>\n",
      "<class 'sqlalchemy.engine.result.ResultProxy'>\n",
      " * Finished step0: done in 47.032s.\n"
     ]
    }
   ],
   "source": [
    "list_of_all_docs = getMimicTexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* list_of_all_docs is a:  <class 'list'>\n",
      "* number docs in list_of_all_docs:  2083180\n",
      "* documents in list_of_all_docs are:  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#### Example ##### \n",
    "print(\"* list_of_all_docs is a: \", type(list_of_all_docs))\n",
    "print(\"* number docs in list_of_all_docs: \", len(list_of_all_docs))\n",
    "print(\"* documents in list_of_all_docs are: \", type(list_of_all_docs[0]))\n",
    "# print(\"* Example documents: \", list_of_all_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Step 2: Create a helper function to run with multiprocessing that will tokenize the document, \n",
    "prepare sentences in the pretty format that BiLM wants (for data.txt), \n",
    "and prepare tokens for Counter create the set of tokens (for vocab.txt)'''\n",
    "\n",
    "def processText(document):\n",
    "    '''\n",
    "    Input: String of the document\n",
    "    Output: {\"tokens\": [list of tokens], \"sentences\": [list of pretty sentences]} = This will output \n",
    "    be used to create vocab.txt and data.txt.\n",
    "    \n",
    "    Data.txt needs > The training data should be randomly split into many \n",
    "        training files, each containing one slice of the data. \n",
    "        Each file contains pre-tokenized and white space separated text, one sentence per line. \n",
    "        Don't include the <S> or </S> tokens in your training data.\n",
    "    \n",
    "    Vocab.txt needs > the vocabulary file should be sorted in descending order by token count in your training data. \n",
    "        The first three lines should be the special tokens (<S>, </S> and <UNK>), then the most common token in the training data, ending with the least common token.\n",
    "\n",
    "    '''\n",
    "    #tokenize\n",
    "    tokenized = n2c2_tokenizer.tokenize_document(document).sentence_tokens_list #list of lists of tokens\n",
    "    #format sentences for data.txt\n",
    "    pretty_sentences = [' '.join(sentences) for sentences in tokenized]\n",
    "    \n",
    "    #flatten the list of lists into one list of strings \n",
    "    flatten = list(itertools.chain(*tokenized))\n",
    "    \n",
    "    return_dict = {'tokens': flatten, 'sentences': pretty_sentences}\n",
    "        \n",
    "    return return_dict\n",
    "\n",
    "unique_words_example = processText(list_of_all_docs[0])\n",
    "#print(unique_words_example[\"set\"])\n",
    "#print(\"#\"*20)\n",
    "#print(unique_words_example[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Step 3: Run the helper function asynchronously with multiprocessing to create the vocab.txt and data.txt \n",
    "that is necessary to run BiLM.\n",
    "'''\n",
    "\n",
    "def createTrainingData(index_start, index_end, n_try):\n",
    "    '''\n",
    "    Input: \n",
    "       * index_start: Int = index of list_of_all_docs you are trying to start from. E.g. 0 would be the first element in the list.\n",
    "       * index_end: Int = index of list_of_all_docs you want to stop on. E.g. -1 would be the whole thing. *I DO NOT RECOMMEND THIS*\n",
    "       * n_try: Int = This will be the suffix on the list of training files you create. I.e. mimmic_data_1.txt, mimic_data_2.txt... \n",
    "    Its going to take a very long time if you try to process all of the Mimic data at once \n",
    "    (there are 2 million + documents in list_of_all_docs).\n",
    "    So instead, we are going to break it up into bite sizes by indexting our list of documents and then \n",
    "    combine the vocabulary Counters to create the vocab.txt at the end.\n",
    "    '''\n",
    "    t1 = time.time() #start the timer\n",
    "    \n",
    "    pool_size = multiprocessing.cpu_count() #NOTE: Usin' all yer CPU's my friend. Change this if you want!!!!\n",
    "    pool = Pool(pool_size)\n",
    "    print('* created worker pools')\n",
    "    results0 = pool.map_async(processText, list_of_all_docs[index_start:index_end]) \n",
    "    print('* initialized map_async to naiveSearchText function with docs')\n",
    "    print('* did map to getSetOfWords function with docs. WITH async')\n",
    "    pool.close()\n",
    "    print('* closed pool')\n",
    "    pool.join()\n",
    "    print('* joined pool')\n",
    "    list_of_dicts = [r for r in results0.get() if r is not None] # A BUNCH OF SETS\n",
    "    print(\"Number of dictionaries created: \", len(list_of_dicts))\n",
    "\n",
    "    \"\"\"Step A: create data.txt: Should have 1 sentence per line\"\"\"\n",
    "    #get all doc's sentences\n",
    "    document_sentences = [s[\"sentences\"]  for s in list_of_dicts]\n",
    "    #flatten to one big list of sentences\n",
    "    flatten_sents: list[string] = list(itertools.chain(*document_sentences))\n",
    "    #output to data_vocab.txt\n",
    "    name_of_file = \"mimic_data_\" + str(n_try) + \".txt\"\n",
    "    with open(name_of_file, \"w\") as out:\n",
    "        for sent in flatten_sents:\n",
    "            out.write(sent)\n",
    "            out.write(\"\\n\")\n",
    "    \n",
    "    \"\"\"Step B: create vocab.txt: Should have 1 token per line, as well as AllenNLP special tokens. \n",
    "    We're going to simply return this output_counter, so that we can sum multiple Counters to output \n",
    "    the vocab.txt\"\"\"\n",
    "    all_tokens = [d[\"tokens\"] for d in list_of_dicts]\n",
    "    all_flatten = list(itertools.chain(*all_tokens))\n",
    "    output_counter = Counter(all_flatten)\n",
    "    \n",
    "    print(\" * Created mimic_data.txt & mimic_vocab.txt: done in %0.3fs.\" % (time.time() - t1))\n",
    "    return output_counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* created worker pools\n",
      "* initialized map_async to naiveSearchText function with docs\n",
      "* did map to getSetOfWords function with docs. WITH async\n",
      "* closed pool\n",
      "* joined pool\n",
      "Number of dictionaries created:  10\n",
      " * Created mimic_data.txt & mimic_vocab.txt: done in 9.051s.\n",
      "* created worker pools\n",
      "* initialized map_async to naiveSearchText function with docs\n",
      "* did map to getSetOfWords function with docs. WITH async\n",
      "* closed pool\n",
      "* joined pool\n",
      "Number of dictionaries created:  10\n",
      " * Created mimic_data.txt & mimic_vocab.txt: done in 3.545s.\n",
      "5159\n"
     ]
    }
   ],
   "source": [
    "# 10 docs in 0.184s.\n",
    "# 100 docs in 0.737s\n",
    "# 1,000 docs in 5.684s.\n",
    "# 10,000 docs in 59.077s.\n",
    "'''Looks like it scales linearly! :) '''\n",
    "#So 100k documents shoud take ~1 hour\n",
    "# 1 million docs should take ~ 10 hours\n",
    "# 2 million shoudl take ~ 20 hours \n",
    "\n",
    "###### A small example #####\n",
    "v1 = createTrainingData(0, 10, 1)\n",
    "v2 = createTrainingData(10, 20, 2)\n",
    "dummy_vocab = v1  + v2\n",
    "print(len(dummy_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now we are going to call createTrainingData() and combine all of of the output_counters into one giant counter for the whole vocab.txt\n",
    "You can train the indices for however much of MIMIC you plan to train on, you'll change these numbers. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##### The whole kitten kaboodle #####\n",
    "first_quarter = (len(list_of_all_docs))*.25\n",
    "print(first_quarter)\n",
    "half_way = (len(list_of_all_docs))*.5\n",
    "print(half_way)\n",
    "three_quarters = (len(list_of_all_docs))*.75\n",
    "print(three_quarters)\n",
    "to_the_end = len(list_of_all_docs)\n",
    "print(to_the_end)\n",
    "\n",
    "\n",
    "output1 = createTrainingData(0, int(first_quarter), 1)  #the third arg of all of these should count sequentially up!\n",
    "output2 = createTrainingData(int(first_quarter), int(half_way), 2)  #second data training file\n",
    "output3 = createTrainingData(int(half_way), int(three_quarters), 3)  #third data training file\n",
    "output4 = createTrainingData(int(three_quarters), int(to_the_end), 4)  #fourth data training file\n",
    "\n",
    "final_vocabulary = output1 + output2 + output3 + output4\n",
    "print(\"* LEN FINAL VOCABULARY (n_tokens_vocab): \", len(final_vocabulary))\n",
    "\n",
    "#We have to add some specific tokens to the top of our vocab.txt to make AllenNLP happy\n",
    "allen_specific = ['<S>','</S>','<UNK>'] \n",
    "\n",
    "#now output to vocab.txt\n",
    "with open(\"mimic_vocab.txt\", \"w\") as out:\n",
    "    for special in allen_specific:\n",
    "        out.write(special)\n",
    "        out.write(\"\\n\")\n",
    "    for token, count in final_vocabulary.most_common():\n",
    "        out.write(token)\n",
    "        out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you are done with this notebook....!!!!\n",
    "there is one **very important** number you need to get out of here! That is the total number of tokens. You will need to set `n_train_tokens` to be equal to this number  +3 for the AllenNLP special tokens in `train_elmo.py`!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" I M P O R T A N T !  !  ! \")\n",
    "print(\"SET THIS TO 'n_train_tokens' in train_elmo.py: \", sum(final_vocabulary.values())+3) #adding 3 for the 3 unique AllenNLP tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train biLM\n",
    "\n",
    "    Prepare input data and a vocabulary file.\n",
    "    --> Train the biLM.\n",
    "    Test (compute the perplexity of) the biLM on heldout data.\n",
    "    Write out the weights from the trained biLM to a hdf5 file.\n",
    "    See the instructions above for using the output from Step #4 in downstream models.\n",
    "\n",
    "Now that the data files have been created, you are ready to train `biLM`!! Please see the `train_on_mimic.sh` script in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other observations about training `BiLM`\n",
    "* make sure you've run python setup.py install!\n",
    "* Double check that you have updated `train_elmo.py` with the `n_train_tokens` (vocab size) for the documents in MIMIC you are using, `n_gpus` your number of GPUs, and your batch size.\n",
    "* Even if you don't have GPU,s n_gpu cannot be set to 0 or training.py will throw an error. \n",
    "* If you see an error like: \n",
    "    \"\"\"\n",
    "     WARNING:tensorflow:Error encountered when serializing lstm_output_embeddings.\n",
    "    Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
    "    'list' object has no attribute 'name'\n",
    "    \"\"\"\n",
    ", it might just be like [this](https://github.com/tflearn/tflearn/issues/190#issuecomment-231545279) and not matter?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
